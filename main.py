import json
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
import torch


class EthicsAssistant:
    def __init__(self, model_name="TinyLlama/TinyLlama-1.1B-Chat-v1.0"):
        self.model_name = model_name
        self.knowledge_base = self.load_knowledge_base('ethics_rules.json')

        print(f"–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ {model_name}...")
        print("(–≠—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–∏–Ω—É—Ç –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –∑–∞–ø—É—Å–∫–µ)")

        self.tokenizer = AutoTokenizer.from_pretrained(
            model_name,
            trust_remote_code=True
        )

        self.model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16,
            device_map="auto",
            low_cpu_mem_usage=True,
            trust_remote_code=True
        )

        self.pipe = pipeline(
            "text-generation",
            model=self.model,
            tokenizer=self.tokenizer,
            device_map="auto"
        )

        print("–ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏ –≥–æ—Ç–æ–≤–∞ –∫ —Ä–∞–±–æ—Ç–µ!")

    def load_knowledge_base(self, file_path: str) -> list:
        """–ó–∞–≥—Ä—É–∂–∞–µ–º –±–∞–∑—É –∑–Ω–∞–Ω–∏–π —Å –ø—Ä–∞–≤–∏–ª–∞–º–∏ —ç—Ç–∏–∫–µ—Ç–∞"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except FileNotFoundError:
            print(f"–§–∞–π–ª {file_path} –Ω–µ –Ω–∞–π–¥–µ–Ω!")
            return []

    def find_relevant_rule(self, user_question: str) -> str:
        """–£–º–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º"""
        user_question_lower = user_question.lower()
        for entry in self.knowledge_base:
            topic = entry['topic'].lower()

            if topic == '–∑–Ω–∞–∫–æ–º—Å—Ç–≤–æ':
                keywords = ['–∑–Ω–∞–∫–æ–º—Å—Ç–≤', '–ø—Ä–µ–¥—Å—Ç–∞', '—Ä—É–∫—É', '–ø–æ–¥–∞—Ç—å', '–ø–æ–∑–Ω–∞–∫–æ–º', '–ø—Ä–µ–¥—Å—Ç–∞–≤']
            elif topic == '–ø–µ—Ä–µ–ø–∏—Å–∫–∞':
                keywords = ['–ø–µ—Ä–µ–ø–∏—Å–∫', '–ø–∏—Å—å–º', 'email', '—Å–æ–æ–±—â–µ–Ω', '–∫–∞–ø—Å', 'caps', '–º–µ—Å—Å–µ–Ω–¥–∂–µ—Ä', 'whatsapp', '—Ç–µ–ª–µ–≥—Ä–∞–º']
            elif topic == '–∑–≤–æ–Ω–æ–∫':
                keywords = ['–∑–≤–æ–Ω–æ–∫', '–ø–æ–∑–≤–æ–Ω–∏—Ç—å', '—Ç–µ–ª–µ—Ñ–æ–Ω', '–∑–≤–æ–Ω–∏—Ç—å', '–ø–æ–∑–¥—Ä–∞–≤', '—Å–æ–∑–≤–æ–Ω–∏—Ç—å']
            elif topic == '—Å–æ—Ü—Å–µ—Ç–∏':
                keywords = ['—Å–æ—Ü—Å–µ—Ç', '—Å–æ—Ü–∏–∞–ª—å–Ω', 'instagram', '–∏–Ω—Å—Ç–∞–≥—Ä–∞–º', 'facebook', '—Ñ–µ–π—Å–±—É–∫', '–≤–∫–æ–Ω—Ç–∞–∫—Ç–µ', 'vk', 'twitter', '—Ç–≤–∏—Ç—Ç–µ—Ä', '—Ñ–æ—Ç–æ', 'photograph', '–ø—É–±–ª–∏–∫–∞—Ü']
            elif topic == '—Ä–∞–∑–≥–æ–≤–æ—Ä':
                keywords = ['—Ä–∞–∑–≥–æ–≤–æ—Ä', '–±–µ—Å–µ–¥', '–æ–±—â–µ–Ω', '—Å–ª—É—à–∞', '–ø–µ—Ä–µ–±–∏–≤', '–¥–∏–∞–ª–æ–≥', '–æ–±—Å—É–∂–¥–µ–Ω']
            elif topic == '–≥–æ—Å—Ç–∏':
                keywords = ['–≥–æ—Å—Ç', '–≤–∏–∑–∏—Ç', '–ø—Ä–∏—Ö–æ–¥', '–ø–æ–¥–∞—Ä–æ–∫', '—Ü–≤–µ—Ç—ã', '–¥–µ—Å–µ—Ä—Ç', '–ø—Ä–∏–≥–ª–∞—à–µ–Ω', '—Ö–æ–∑—è–π–∫']
            else:
                keywords = [topic]

            if any(keyword in user_question_lower for keyword in keywords):
                return entry['rule']

        return None

    def generate_response(self, user_question: str) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–æ–¥–µ–ª–∏"""
        relevant_rule = self.find_relevant_rule(user_question)

        if not relevant_rule:
            return "–ò–∑–≤–∏–Ω–∏—Ç–µ, –≤ –º–æ–µ–π –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π –Ω–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –ø–æ —ç—Ç–æ–º—É –≤–æ–ø—Ä–æ—Å—É."

        if "TinyLlama" in self.model_name:
            prompt = f"""<|system|>
            –¢—ã –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –ø–æ —ç—Ç–∏–∫–µ—Ç—É. –û—Ç–≤–µ—á–∞–π –¢–û–õ–¨–ö–û –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–≥–æ –ø—Ä–∞–≤–∏–ª–∞.
            –ù–µ –¥–æ–±–∞–≤–ª—è–π –Ω–∏—á–µ–≥–æ –æ—Ç —Å–µ–±—è. –û—Ç–≤–µ—á–∞–π –∫—Ä–∞—Ç–∫–æ –∏ –ø–æ –¥–µ–ª—É.</s>
            <|user|>
            –ü–†–ê–í–ò–õ–û: {relevant_rule}
            –í–û–ü–†–û–°: {user_question}
            –û–¢–í–ï–¢–ê–ô –¢–û–õ–¨–ö–û –ù–ê –û–°–ù–û–í–ï –ü–†–ê–í–ò–õ–ê –í–´–®–ï.</s>
            <|assistant|>
            –ù–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–∞–≤–∏–ª–∞:"""
        else:
            # –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è –¥—Ä—É–≥–∏—Ö –º–æ–¥–µ–ª–µ–π
            prompt = f"""–¢—ã –∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç –ø–æ —ç—Ç–∏–∫–µ—Ç—É. –û—Ç–≤–µ—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å —Å—Ç—Ä–æ–≥–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–≥–æ –ø—Ä–∞–≤–∏–ª–∞.
            –ù–µ –ø—Ä–∏–¥—É–º—ã–≤–∞–π –Ω–∏—á–µ–≥–æ –æ—Ç —Å–µ–±—è. –ë—É–¥—å —Ç–æ—á–Ω—ã–º –∏ –ª–∞–∫–æ–Ω–∏—á–Ω—ã–º.

            –ü—Ä–∞–≤–∏–ª–æ: {relevant_rule}
            –í–æ–ø—Ä–æ—Å: {user_question}

            –û—Ç–≤–µ—Ç:"""

        try:
            outputs = self.pipe(
                prompt,
                max_new_tokens=150,
                do_sample=True,
                temperature=0.3,
                top_p=0.9,
                pad_token_id=self.tokenizer.eos_token_id
            )

            full_response = outputs[0]['generated_text']
            response = full_response.replace(prompt, "").strip()
            return response

        except Exception as e:
            return f"–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞: {str(e)}"


def main():
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞ —Å –æ—Ç–∫—Ä—ã—Ç–æ–π –º–æ–¥–µ–ª—å—é
    assistant = EthicsAssistant("TinyLlama/TinyLlama-1.1B-Chat-v1.0")

    print("\nü§µ –≠—Ç–∏–∫–µ—Ç-–∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ!")
    print("–ó–∞–¥–∞–π—Ç–µ –≤–æ–ø—Ä–æ—Å –ø–æ —ç—Ç–∏–∫–µ—Ç—É (–∏–ª–∏ '–≤—ã—Ö–æ–¥' –¥–ª—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è):")

    while True:
        user_input = input("\n–í–∞—à –≤–æ–ø—Ä–æ—Å: ").strip()

        if user_input.lower() in ['–≤—ã—Ö–æ–¥', 'exit', 'quit']:
            print("–î–æ —Å–≤–∏–¥–∞–Ω–∏—è!")
            break

        if not user_input:
            continue

        response = assistant.generate_response(user_input)
        print(f"\n–û—Ç–≤–µ—Ç: {response}")


if __name__ == "__main__":
    main()
