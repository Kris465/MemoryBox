# Большие LLM модели или локальные модели?

## Зачем нужны локальные нейронные сети вместо больших языковых моделей?

**Введение**

На прошлом уроке мы создавали ИИ агента с помощью API Mistral.

Сегодня мы поговорим о том, почему в некоторых случаях локальные (малые) нейронные сети предпочтительнее больших языковых моделей (LLM, Large Language Models), таких как GPT-4 или Claude.  

Вы уже знаете, что большие модели демонстрируют впечатляющие возможности в генерации текста, анализе данных и решении сложных задач. Однако у них есть серьёзные ограничения, и иногда локальные нейросети оказываются более эффективными.  

### **1. Конфиденциальность и безопасность данных**  
**Проблема больших LLM:**  
- Когда вы используете облачные модели (ChatGPT, Gemini и др.), ваши данные отправляются на серверы компании-разработчика.  
- Это может быть неприемлемо для работы с персональными, медицинскими или корпоративными данными.  

**Преимущество локальных моделей:**  
- Данные остаются на вашем устройстве.  
- Нет риска утечки или несанкционированного доступа.  
- Подходит для GDPR, HIPAA и других строгих стандартов.  

**Пример:** Врач не может загружать истории болезней в ChatGPT, но может использовать локальную модель для анализа.  

Новостные статьи:

https://iz.ru/1615227/dmitrii-bulgakov/taina-v-perepiske-eksperty-nashli-kliuch-k-lichnym-dannym-iz-chatgpt

https://safe.cnews.ru/news/top/2025-04-07_ukrainskij_haker_vzlamyval

https://xakep.ru/2023/11/30/chatgpt-pii/

### **2. Скорость работы и автономность**  
**Проблема больших LLM:**  
- Для работы требуется стабильный интернет.  
- Задержки из-за передачи запросов на сервер и обратно.  

**Преимущество локальных моделей:**  
- Работают оффлайн.  
- Быстрый отклик (не нужно ждать ответа из облака).  
- Можно использовать в полевых условиях, на удалённых объектах.  

**Пример:** Переводчик в зоне без интернета, чат-бот на заводском оборудовании.  

Мы в прошлый раз писали ИИ-агента используя API Mistral, а сможет ли он функционировать без интернета и следовательно доступа к API? В итоге, если будут блокировать доступ к иностранным ресурсам, сможет ли наш сервис работать?

### **3. Энергоэффективность и стоимость**  
**Проблема больших LLM:**  
- Обучение и запуск GPT-4 требует огромных вычислительных ресурсов.  
- Использование API платное (например, OpenAI берёт деньги за количество запросов).  

**Преимущество локальных моделей:**  
- Малые модели работают даже на ноутбуке или Raspberry Pi.  
- Нет затрат на облачные сервисы.  
- Экономия на масштабировании (если нужно 1000 запросов в день, локальная модель дешевле).  

**Пример:** Студенческий проект или стартап с ограниченным бюджетом.  

Статьи:

https://habr.com/ru/articles/780092/


### **4. Кастомизация и специализация**  
**Проблема больших LLM:**  
- GPT-4 — модель общего назначения, её сложно адаптировать под узкие задачи.  
- Fine-tuning (дообучение) больших моделей дорого и требует экспертизы.  

**Преимущество локальных моделей:**  
- Можно дообучить под конкретную задачу (например, анализ юридических документов или медицинских записей).  
- Лёгкая интеграция в существующие системы.  

**Пример:**  
- Юридическая фирма обучает локальную модель на своих документах.  
- Игровой ИИ, который должен работать предсказуемо, а не "фантазировать" как ChatGPT.  

### **5. Предсказуемость и контроль**  
**Проблема больших LLM:**  
- GPT-4 может "галлюцинировать" (придумывать факты).  
- Поведение модели зависит от разработчика (OpenAI может менять правила фильтрации).  

**Преимущество локальных моделей:**  
- Полный контроль над логикой работы.  
- Нет неожиданных обновлений, которые сломают ваш продукт.  
- Можно жёстко ограничить тематику (например, чтобы ИИ не отклонялся от темы).  

**Пример:** Финансовый советник на основе локальной модели не станет шутить или давать рискованные советы.  

## Параметры локальных сетей и настройка их в LM Studio.

Мы уже разбирали параметры нейронных сетей на втором занятии. Перед повторением и практическим применением давайте освежим в памяти:

https://www.promptingguide.ai/ru/introduction/settings

Вы уже скачали небольшую модель (например, **Phi-3, Mistral, Llama 2 или Gemma3**), но её поведение можно гибко регулировать, меняя параметры генерации. Это важно, чтобы:  
- Улучшить качество ответов.  
- Контролировать креативность/консервативность модели.  
- Оптимизировать скорость работы.

## **1. Основные параметры генерации текста**  

В **LM Studio** (и других аналогичных инструментах) настройки обычно находятся во вкладке **"Model Settings"** или **"Generation Parameters"**.  

### **Temperature (Температура)**
**Что делает?**  
Управляет **случайностью** ответов.  

- **Низкое значение (0.1–0.5)** → ответы более предсказуемые, консервативные.  
- **Высокое значение (0.7–1.2)** → ответы более разнообразные, креативные (но могут быть менее точными).  

**Когда менять?**  
- Если нужны **фактические ответы** (например, пересказ лекции) → `0.3–0.5`.  
- Если нужны **креативные тексты** (стихи, идеи, диалоги) → `0.7–1.0`.

### **Top-p (Nucleus Sampling)**  
**Что делает?**  
Ограничивает выбор слов только из наиболее вероятных вариантов.  

- **Низкое значение (0.5–0.8)** → модель выбирает только самые вероятные слова (более строгие ответы).  
- **Высокое значение (0.9–1.0)** → модель может использовать менее очевидные варианты.  

**Когда менять?**  
- Для **технических/научных текстов** → `0.7–0.9`.  
- Для **художественных текстов** → `0.9–1.0`.  

### **Top-k**  
**Что делает?**  
Ограничивает количество вариантов, из которых модель выбирает следующее слово.  

- **Top-k = 10** → модель выбирает из 10 самых вероятных слов.  
- **Top-k = 50** → больше разнообразия, но выше шанс ошибок.  

**Когда менять?**  
- Если модель **слишком часто ошибается** → уменьшить (например, `20`).  
- Если ответы **слишком шаблонные** → увеличить (например, `50`).

### **Max Length (Максимальная длина ответа)**  
**Что делает?**  
Определяет, сколько **токенов** (частей слов) может сгенерировать модель.  

- **Слишком мало (128)** → обрывает ответы на полуслове.  
- **Слишком много (2048)** → может тормозить и "болтать" без конца.  

**Оптимальные значения:**  
- Для коротких ответов → `256–512`.  
- Для больших текстов → `1024–2048` (если позволяет видеокарта).

### **Repeat Penalty (Штраф за повторения)**  
**Что делает?**  
Помогает избежать **зацикливания** (когда модель повторяет одни и те же фразы).  

- **Обычное значение** → `1.1–1.3`.  
- Если модель **повторяется** → увеличить до `1.5–2.0`.

## **2. Дополнительные настройки в LM Studio**  

### **Контекстное окно (Context Size)**  
**Что делает?**  
Определяет, сколько **предыдущего текста** модель "помнит" во время генерации.  

- **Маленькие модели (7B и меньше)** → обычно `2048–4096`.  
- **Большие модели (13B+)** → можно пробовать `8192`.  

**Важно:** Чем больше контекст, тем больше требуется оперативной памяти!

### **Batch Size (Размер пакета)**  
**Что делает?**  
Определяет, сколько данных обрабатывается за один проход.  

- **Увеличение** → ускоряет работу, но требует больше видеопамяти.  
- **Уменьшение** → медленнее, но стабильнее на слабых ПК.  

**Рекомендация:** Оставить **по умолчанию**, если нет проблем с производительностью.

### **GPU/CPU Offloading**  
**Что делает?**  
Решает, какая часть вычислений идёт на **видеокарту (GPU)**, а какая — на **процессор (CPU)**.  

- **Полный GPU** → быстрее, но требует мощной видеокарты.  
- **Частичный CPU** → медленнее, но работает на слабых ПК.  

**Как настроить в LM Studio?**  
- `Settings → GPU Layers` → можно указать, сколько слоёв нейросети загружать на видеокарту.

## **3. Практические рекомендации**  

### **Стартовые настройки для разных задач**  

| Задача               | Temperature | Top-p | Top-k | Max Length |  
|----------------------|-------------|-------|-------|------------|  
| Технические ответы   | 0.3–0.5     | 0.7–0.9 | 20–40 | 512       |  
| Креативные тексты    | 0.7–1.0     | 0.9–1.0 | 40–60 | 1024      |  
| Чат-бот (диалоги)    | 0.5–0.7     | 0.8–0.95 | 30–50 | 768       |  

### **Что делать, если модель...**  
- **"Галлюцинирует" (придумывает факты)?** → Уменьшить `Temperature` и `Top-p`.  
- **Повторяет одни и те же фразы?** → Увеличить `Repeat Penalty`.  
- **Тормозит?** → Уменьшить `Max Length` и `Context Size`.

## **Простое дообучение локальной модели в LM Studio (наглядный пример)**  

**Введение**  
Сейчас мы разберём **как дообучить (fine-tune) маленькую языковую модель** под конкретную задачу прямо в **LM Studio**. Это полезно, если нужно:  
- Научить модель говорить в определённом стиле (например, как Шекспир).  
- Сделать её экспертом в узкой области (медицина, юриспруденция).  
- Исправить её типичные ошибки.  

Так как LM Studio не поддерживает полноценное дообучение (как, например, **llama.cpp** или **Oobabooga**), мы воспользуемся **контекстным обучением (in-context learning)** — это простой способ "подсказать" модели, как отвечать, без изменения её весов.

## **1. Подготовка данных**  
Для дообучения нам понадобится:  
1. **Текстовый файл** (`.txt` или `.jsonl`) с примерами вопросов и ответов.  
2. **Чёткая инструкция** для модели.  

**Пример данных (`shakespeare.txt`):**  
```text
[Instruction] Отвечай в стиле Шекспира.  
[Q] Как дела?  
[A] О, благородный друг, мои дела — как пение птиц на рассвете, превосходны!  

[Q] Что думаешь о погоде?  
[A] Сей день окутан туманом, словно тайны моей души!  

[Q] Как пройти в библиотеку?  
[A] Ступай на восток, мимо рыночной площади, ибо там хранится мудрость веков!  
```  

*(Чем больше примеров, тем лучше!)*

## **2. Загрузка данных в LM Studio**

1. **Откройте LM Studio** и загрузите свою модель (например, **Phi-3** или **Mistral-7B**).
2. Перейдите во вкладку **"Chat"**.  
3. **Вставьте ваш файл** с примерами в системный промпт (или загрузите через `Load Prompt`).  

**Пример системного промпта:**  
```text
Ты — Шекспировский помощник. Всегда отвечай в стиле Уильяма Шекспира, используя старинные слова и метафоры.  

Примеры:  
- На вопрос "Как дела?" отвечай: "О, благородный друг, мои дела — как пение птиц на рассвете!"  
- На вопрос о погоде: "Сей день окутан туманом, словно тайны моей души!"  
```  

*(Это **контекстное обучение** — модель не меняется, но подстраивается под примеры.)*

## **3. Проверка работы**  
Теперь спросите что-то вроде:  
- *"Что ты думаешь о любви?"*  
- *"Как мне приготовить ужин?"*  

**Модель должна ответить в шекспировском стиле!**  

*(Если ответы недостаточно точные — добавьте больше примеров в промпт.)*

## **4. Альтернатива: Лёгкое дообучение через "LoRA"**  
Если нужно **реальное дообучение** (изменение весов модели), LM Studio не поддерживает это напрямую, но можно:

1. **Использовать Ollama** (`ollama pull mistral`) + Modelfiles.

2. **Или llama.cpp** + набор данных в `.jsonl`.  


**Пример Modelfile для Ollama:**  
```text
FROM mistral  
SYSTEM "Ты — Шекспировский помощник..."  
PARAMETER temperature 0.7  
TEMPLATE """[Q] {{.Input}} [A] {{.Output}}"""  
```  

*(Но это будет на следующем уроке или по требованию)*

## **5. Выводы**  
- **Контекстное обучение** (через промпт) — самый простой способ "дообучить" модель в LM Studio.  
- **Настоящее дообучение** (LoRA, fine-tuning) сложнее и требует других инструментов.  
- **Чем больше примеров** — тем лучше качество подстройки.  

**Практическое задание:**  
1. Создайте файл с 5–10 примерами (например, "отвечай как пират" или "объясняй сложные темы просто").  
2. Загрузите его в LM Studio и проверьте, как меняются ответы модели.  
