import os
import asyncio
import json
import numpy as np
from sentence_transformers import SentenceTransformer, util
from aiogram import Bot, Dispatcher, F
from aiogram.filters import Command
from aiogram.types import Message
from dotenv import load_dotenv

load_dotenv()


class EthicsAssistant:
    def __init__(self):
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–∞–ª–µ–Ω—å–∫—É—é –º–æ–¥–µ–ª—å –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ (–≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π)
        print("–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –¥–ª—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è...")
        self.embedding_model = SentenceTransformer(
            'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')
        self.knowledge_base = self.load_knowledge_base('ethics_rules.json')
        self.setup_knowledge_embeddings()
        print("–ú–æ–¥–µ–ª—å –¥–ª—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è –∑–∞–≥—Ä—É–∂–µ–Ω–∞!")

    def load_knowledge_base(self, file_path: str) -> list:
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except FileNotFoundError:
            print(f"–§–∞–π–ª {file_path} –Ω–µ –Ω–∞–π–¥–µ–Ω!")
            return []

    def setup_knowledge_embeddings(self):
        """–°–æ–∑–¥–∞–µ–º –≤–µ–∫—Ç–æ—Ä–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –¥–ª—è –≤—Å–µ—Ö –ø—Ä–∞–≤–∏–ª"""
        self.rules_texts = []
        self.rules_embeddings = []

        for entry in self.knowledge_base:
            rule_variants = [
                entry['topic'],
                f"–ø—Ä–∞–≤–∏–ª–∞ {entry['topic']}",
                f"—ç—Ç–∏–∫–µ—Ç {entry['topic']}",
                f"–∫–∞–∫ –≤–µ—Å—Ç–∏ —Å–µ–±—è {entry['topic']}",
                entry['rule'][:100]
            ]

            for variant in rule_variants:
                self.rules_texts.append({
                    'topic': entry['topic'],
                    'rule': entry['rule'],
                    'variant': variant
                })

        if self.rules_texts:
            texts_to_embed = [item['variant'] for item in self.rules_texts]
            self.rules_embeddings = self.embedding_model.encode(
                texts_to_embed, convert_to_tensor=True)

    def find_relevant_rule_semantic(self, user_question: str) -> str:
        """–ü–æ–∏—Å–∫ –ø—Ä–∞–≤–∏–ª–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ similarity"""
        if not self.rules_texts:
            return None

        question_embedding = self.embedding_model.encode(
            user_question, convert_to_tensor=True)

        cos_scores = util.cos_sim(question_embedding, self.rules_embeddings)[0]

        best_match_idx = np.argmax(cos_scores.cpu().numpy())
        best_score = cos_scores[best_match_idx].item()

        if best_score > 0.3:
            best_match = self.rules_texts[best_match_idx]
            return best_match['rule']

        return None

    def find_relevant_rule_hybrid(self, user_question: str) -> str:
        """–ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫: —Å–Ω–∞—á–∞–ª–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π, –ø–æ—Ç–æ–º –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º"""
        rule = self.find_relevant_rule_semantic(user_question)
        if rule:
            return rule

        user_question_lower = user_question.lower()
        for entry in self.knowledge_base:
            topic = entry['topic'].lower()

            if topic == '–∑–Ω–∞–∫–æ–º—Å—Ç–≤–æ':
                keywords = ['–∑–Ω–∞–∫–æ–º—Å—Ç–≤', '–ø—Ä–µ–¥—Å—Ç–∞', '—Ä—É–∫—É', '–ø–æ–¥–∞—Ç—å',
                            '–ø–æ–∑–Ω–∞–∫–æ–º']
            elif topic == '–ø–µ—Ä–µ–ø–∏—Å–∫–∞':
                keywords = ['–ø–µ—Ä–µ–ø–∏—Å–∫', '–ø–∏—Å—å–º', 'email', '—Å–æ–æ–±—â–µ–Ω', '–∫–∞–ø—Å',
                            'caps']
            elif topic == '–∑–≤–æ–Ω–æ–∫':
                keywords = ['–∑–≤–æ–Ω–æ–∫', '–ø–æ–∑–≤–æ–Ω–∏—Ç—å', '—Ç–µ–ª–µ—Ñ–æ–Ω', '–∑–≤–æ–Ω–∏—Ç—å']
            elif topic == '—Å–æ—Ü—Å–µ—Ç–∏':
                keywords = ['—Å–æ—Ü—Å–µ—Ç', '—Å–æ—Ü–∏–∞–ª—å–Ω', 'instagram', '–∏–Ω—Å—Ç–∞–≥—Ä–∞–º',
                            '—Ñ–æ—Ç–æ']
            elif topic == '—Ä–∞–∑–≥–æ–≤–æ—Ä':
                keywords = ['—Ä–∞–∑–≥–æ–≤–æ—Ä', '–±–µ—Å–µ–¥', '–æ–±—â–µ–Ω', '—Å–ª—É—à–∞', '–ø–µ—Ä–µ–±–∏–≤']
            elif topic == '–≥–æ—Å—Ç–∏':
                keywords = ['–≥–æ—Å—Ç', '–≤–∏–∑–∏—Ç', '–ø—Ä–∏—Ö–æ–¥', '–ø–æ–¥–∞—Ä–æ–∫', '—Ü–≤–µ—Ç—ã']
            else:
                keywords = [topic]

            if any(keyword in user_question_lower for keyword in keywords):
                return entry['rule']

        return None

    def generate_response(self, user_question: str) -> str:
        """–ü—Ä–æ—Å—Ç–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –Ω–∞–π–¥–µ–Ω–Ω–æ–µ –ø—Ä–∞–≤–∏–ª–æ –±–µ–∑ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏"""
        relevant_rule = self.find_relevant_rule_hybrid(user_question)

        if not relevant_rule:
            return "–ò–∑–≤–∏–Ω–∏—Ç–µ, –≤ –º–æ–µ–π –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π –Ω–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –ø–æ —ç—Ç–æ–º—É –≤–æ–ø—Ä–æ—Å—É. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –∑–∞–¥–∞—Ç—å –≤–æ–ø—Ä–æ—Å –ø–æ-–¥—Ä—É–≥–æ–º—É."

        return f"üìñ –ü—Ä–∞–≤–∏–ª–æ —ç—Ç–∏–∫–µ—Ç–∞:\n\n{relevant_rule}"


assistant = EthicsAssistant()

bot = Bot(token=os.getenv('TOKEN'))
dp = Dispatcher()


@dp.message(Command("start"))
async def cmd_start(message: Message):
    welcome_text = """
    ü§µ –≠—Ç–∏–∫–µ—Ç-–ö–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç –ø—Ä–∏–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –≤–∞—Å!

    –Ø –ø–æ–º–æ–≥—É –≤–∞–º —Å –ø—Ä–∞–≤–∏–ª–∞–º–∏ —ç—Ç–∏–∫–µ—Ç–∞:
    ‚Ä¢ –ó–Ω–∞–∫–æ–º—Å—Ç–≤–æ –∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ
    ‚Ä¢ –î–µ–ª–æ–≤–∞—è –ø–µ—Ä–µ–ø–∏—Å–∫–∞  
    ‚Ä¢ –¢–µ–ª–µ—Ñ–æ–Ω–Ω—ã–µ –∑–≤–æ–Ω–∫–∏
    ‚Ä¢ –ü–æ–≤–µ–¥–µ–Ω–∏–µ –≤ —Å–æ—Ü—Å–µ—Ç—è—Ö
    ‚Ä¢ –ö—É–ª—å—Ç—É—Ä–∞ –æ–±—â–µ–Ω–∏—è
    ‚Ä¢ –ü—Ä–∏–µ–º –≥–æ—Å—Ç–µ–π

    –ü—Ä–æ—Å—Ç–æ –∑–∞–¥–∞–π—Ç–µ –≤–æ–ø—Ä–æ—Å, –Ω–∞–ø—Ä–∏–º–µ—Ä:
    "–ö–∞–∫ –≤–µ—Å—Ç–∏ —Å–µ–±—è –ø—Ä–∏ –∑–Ω–∞–∫–æ–º—Å—Ç–≤–µ?"
    "–ß—Ç–æ –Ω–µ–ª—å–∑—è –¥–µ–ª–∞—Ç—å –≤ –ø–µ—Ä–µ–ø–∏—Å–∫–µ?"
    "–ö–∞–∫ –ø—Ä–∞–≤–∏–ª—å–Ω–æ –∑–≤–æ–Ω–∏—Ç—å?"
    "–ö–∞–∫ –≤–µ—Å—Ç–∏ —Å–µ–±—è –≤ —Å–æ—Ü—Å–µ—Ç—è—Ö?"
    """
    await message.answer(welcome_text)


@dp.message(Command("help"))
async def cmd_help(message: Message):
    help_text = """
    –ü–æ–º–æ—â—å –ø–æ –±–æ—Ç—É:

    –î–æ—Å—Ç—É–ø–Ω—ã–µ –∫–æ–º–∞–Ω–¥—ã:
    /start - –Ω–∞—á–∞—Ç—å —Ä–∞–±–æ—Ç—É
    /help - –ø–æ–∫–∞–∑–∞—Ç—å —Å–ø—Ä–∞–≤–∫—É
    /topics - –ø–æ–∫–∞–∑–∞—Ç—å —Ç–µ–º—ã

    –ù–∞–ø–∏—à–∏—Ç–µ –≤–æ–ø—Ä–æ—Å –æ –ø—Ä–∞–≤–∏–ª–∞—Ö —ç—Ç–∏–∫–µ—Ç–∞!
    """
    await message.answer(help_text)


@dp.message(Command("topics"))
async def cmd_topics(message: Message):
    topics = []
    for entry in assistant.knowledge_base:
        topics.append(f"‚Ä¢ {entry['topic'].capitalize()}")

    topics_text = "üìö –î–æ—Å—Ç—É–ø–Ω—ã–µ —Ç–µ–º—ã:\n\n" + "\n".join(topics)
    await message.answer(topics_text)


@dp.message(F.text)
async def handle_message(message: Message):
    user_question = message.text.strip()

    if len(user_question) < 3 or user_question.startswith('/'):
        return

    await message.bot.send_chat_action(message.chat.id, "typing")
    await asyncio.sleep(0.5)

    response = assistant.generate_response(user_question)
    await message.answer(response)


async def main():
    print("–ë–æ—Ç –∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è...")
    await dp.start_polling(bot)


if __name__ == "__main__":
    asyncio.run(main())
